{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifteen-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adopted-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['01_palm',\n",
    " '02_l',\n",
    " '03_fist',\n",
    " '04_fist_moved',\n",
    " '05_thumb',\n",
    " '06_index',\n",
    " '07_ok',\n",
    " '08_palm_moved',\n",
    " '09_c',\n",
    " '10_down']\n",
    "\n",
    "#%%\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "#%%\n",
    "def normaliz_data(np_data):\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    scaled_images  = normaliz_data2(np_data)\n",
    "    scaled_images  = np_data.reshape(-1, 120, 320, 1)\n",
    "    return scaled_images\n",
    "\n",
    "#%%\n",
    "def normaliz_data2(v):\n",
    "    normalized_v = v / np.sqrt(np.sum(v**2))\n",
    "    return normalized_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ordinary-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model(\"HGR_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wired-actress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe =  02_l Precision =  100.0 %\n",
      "Classe =  02_l Precision =  100.0 %\n",
      "Classe =  01_palm Precision =  100.0 %\n",
      "Classe =  01_palm Precision =  100.0 %\n",
      "Classe =  01_palm Precision =  100.0 %\n",
      "Classe =  09_c Precision =  100.0 %\n",
      "Classe =  01_palm Precision =  100.0 %\n",
      "Classe =  02_l Precision =  100.0 %\n",
      "Classe =  02_l Precision =  100.0 %\n",
      "Classe =  02_l Precision =  100.0 %\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-899d9ef5b04f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#print(frame_to_predict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_to_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mclasse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Classe = '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Precision = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "to_predict = []\n",
    "num_frames = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "classe =''\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    to_predict.append(cv2.resize(gray, (120, 320)))\n",
    "\n",
    "\n",
    "    if len(to_predict) == 120:\n",
    "        frame_to_predict = np.array(to_predict, dtype=np.float32)\n",
    "        frame_to_predict = normaliz_data(frame_to_predict)\n",
    "        #print(frame_to_predict)\n",
    "        predict = new_model.predict(frame_to_predict)\n",
    "        classe = classes[np.argmax(predict)]\n",
    "\n",
    "        print('Classe = ',classe, 'Precision = ', np.amax(predict)*100,'%')\n",
    "\n",
    "\n",
    "        #print(frame_to_predict)\n",
    "        to_predict = []\n",
    "        #sleep(0.1) # Time in seconds\n",
    "        #font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame, classe, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0),1,cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Hand Gesture Recognition',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-therapist",
   "metadata": {},
   "source": [
    "# Palm Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moved-remove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# organize imports\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "\n",
    "# global variables\n",
    "bg = None\n",
    "\n",
    "\n",
    "def run_avg(image, aWeight):\n",
    "    global bg\n",
    "    # initialize the background\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return\n",
    "\n",
    "    # compute weighted average, accumulate it and update the background\n",
    "    cv2.accumulateWeighted(image, bg, aWeight)\n",
    "\n",
    "\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff,\n",
    "                                threshold,\n",
    "                                255,\n",
    "                                cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(),\n",
    "                                 cv2.RETR_EXTERNAL,\n",
    "                                 cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialize weight for running average\n",
    "    aWeight = 0.5\n",
    "\n",
    "    # get the reference to the webcam\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    # region of interest (ROI) coordinates\n",
    "    top, right, bottom, left = 10, 350, 225, 590\n",
    "\n",
    "    # initialize num of frames\n",
    "    num_frames = 0\n",
    "    image_num = 0\n",
    "\n",
    "    start_recording = False\n",
    "\n",
    "    # keep looping, until interrupted\n",
    "    while(True):\n",
    "        # get the current frame\n",
    "        (grabbed, frame) = camera.read()\n",
    "        if (grabbed == True):\n",
    "\n",
    "            # resize the frame\n",
    "            frame = imutils.resize(frame, width=700)\n",
    "\n",
    "            # flip the frame so that it is not the mirror view\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # clone the frame\n",
    "            clone = frame.copy()\n",
    "\n",
    "            # get the height and width of the frame\n",
    "            (height, width) = frame.shape[:2]\n",
    "\n",
    "            # get the ROI\n",
    "            roi = frame[top:bottom, right:left]\n",
    "\n",
    "            # convert the roi to grayscale and blur it\n",
    "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "            # to get the background, keep looking till a threshold is reached\n",
    "            # so that our running average model gets calibrated\n",
    "            if num_frames < 30:\n",
    "                run_avg(gray, aWeight)\n",
    "                print(num_frames)\n",
    "            else:\n",
    "                # segment the hand region\n",
    "                hand = segment(gray)\n",
    "\n",
    "                # check whether hand region is segmented\n",
    "                if hand is not None:\n",
    "                    # if yes, unpack the thresholded image and\n",
    "                    # segmented region\n",
    "                    (thresholded, segmented) = hand\n",
    "\n",
    "                    # draw the segmented region and display the frame\n",
    "                    cv2.drawContours(\n",
    "                        clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "                    if start_recording:\n",
    "\n",
    "                        # Mention the directory in which you wanna store the images followed by the image name\n",
    "                        cv2.imwrite(\"Dataset/FistTest/fist_\" +\n",
    "                                    str(image_num) + '.png', thresholded)\n",
    "                        image_num += 1\n",
    "                    cv2.imshow(\"Thesholded\", thresholded)\n",
    "\n",
    "            # draw the segmented hand\n",
    "            cv2.rectangle(clone, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "            # increment the number of frames\n",
    "            num_frames += 1\n",
    "\n",
    "            # display the frame with segmented hand\n",
    "            cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "            # observe the keypress by the user\n",
    "            keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            # if the user pressed \"q\", then stop looping\n",
    "            if keypress == ord(\"q\") or image_num > 100:\n",
    "                # free up memory\n",
    "                camera.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "            if keypress == ord(\"s\"):\n",
    "                start_recording = True\n",
    "\n",
    "        else:\n",
    "            print(\"[Warning!] Error input, Please check your(camra Or video)\")\n",
    "            break\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-miniature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MainEnv",
   "language": "python",
   "name": "mainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
